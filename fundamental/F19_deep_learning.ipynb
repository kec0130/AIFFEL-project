{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Tensorflow-기반-분류-모델\" data-toc-modified-id=\"1.-Tensorflow-기반-분류-모델-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Tensorflow 기반 분류 모델</a></span></li><li><span><a href=\"#2.-Parameters/Weights\" data-toc-modified-id=\"2.-Parameters/Weights-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Parameters/Weights</a></span></li><li><span><a href=\"#3.-활성화-함수-(Activation-Functions)\" data-toc-modified-id=\"3.-활성화-함수-(Activation-Functions)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. 활성화 함수 (Activation Functions)</a></span></li><li><span><a href=\"#4.-손실함수-(Loss-Functions)\" data-toc-modified-id=\"4.-손실함수-(Loss-Functions)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. 손실함수 (Loss Functions)</a></span></li><li><span><a href=\"#5.-경사하강법(Gradient-Descent)\" data-toc-modified-id=\"5.-경사하강법(Gradient-Descent)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. 경사하강법(Gradient Descent)</a></span></li><li><span><a href=\"#6.-오차역전파법(Backpropagation)\" data-toc-modified-id=\"6.-오차역전파법(Backpropagation)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>6. 오차역전파법(Backpropagation)</a></span></li><li><span><a href=\"#7.-모델-학습-Step-by-Step\" data-toc-modified-id=\"7.-모델-학습-Step-by-Step-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>7. 모델 학습 Step-by-Step</a></span></li><li><span><a href=\"#8.-추론-과정-구현과-정확도(Accuracy)-계산\" data-toc-modified-id=\"8.-추론-과정-구현과-정확도(Accuracy)-계산-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>8. 추론 과정 구현과 정확도(Accuracy) 계산</a></span></li><li><span><a href=\"#9.-전체-학습-사이클-수행\" data-toc-modified-id=\"9.-전체-학습-사이클-수행-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>9. 전체 학습 사이클 수행</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 들여다보기 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensorflow 기반 분류 모델\n",
    "딥러닝 프레임워크를 이용하면 몇 줄 안되는 코드만으로 MNIST 데이터셋을 99% 이상의 정확도로 분류할 수 있는 이미지 분류기를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5001 - accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2288 - accuracy: 0.9357\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1803 - accuracy: 0.9481\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.9571\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1311 - accuracy: 0.9627\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.9671\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1032 - accuracy: 0.9704\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0929 - accuracy: 0.9737\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0843 - accuracy: 0.9762\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0768 - accuracy: 0.9783\n",
      "313/313 - 0s - loss: 0.1096 - accuracy: 0.9663\n",
      "test_loss: 0.10961394011974335 \n",
      "test_accuracy: 0.9663000106811523\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameters/Weights\n",
    "입력층-은닉층, 은닉층-출력층 사이에는 사실 각각 행렬(Matrix)이 존재합니다. 이 행렬들을 Parameter 혹은 Weight라고 부릅니다. 위에서 간단히 만들어 보았던 MLP 기반 딥러닝 모델을 Numpy로 다시 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8114015 ,  1.4604368 , -0.39148254,  0.8063032 ,  0.18500812,\n",
       "        0.84003435, -0.96561598, -1.02460928, -0.24127622, -1.00904824,\n",
       "        1.66344866, -0.02463303, -0.11396312,  0.01121023,  1.29985291,\n",
       "       -1.93527165, -0.81867207,  1.11320748,  0.27245324,  0.42728084,\n",
       "       -0.12689856,  0.666382  ,  0.14207089, -1.54413387, -1.10795256,\n",
       "        1.31873945, -1.07247424,  0.02491284,  0.24265037,  1.00611531,\n",
       "       -0.82825838, -0.75918636,  1.84139569, -0.33178146, -1.75528499,\n",
       "        0.65556802,  1.53399435,  0.62857641,  1.59802367,  1.15548845,\n",
       "       -0.59763959, -1.16752358,  0.34375281, -0.22035542, -0.21043798,\n",
       "       -0.35985237,  0.3784646 ,  0.12397754,  0.58014228, -1.45029916])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해 봅시다. 50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 활성화 함수 (Activation Functions)\n",
    "활성화 함수는 보통 비선형 함수를 사용하는데, 레이어 사이에 이 비선형 함수가 포함되지 않은 MLP는 한개의 레이어로 이루어진 모델과 수학적으로 다른 점이 없습니다. 따라서 딥러닝에서는 이 활성화 함수의 존재가 필수적입니다. 첫 번째 은닉층의 출력 a1에다가 sigmoid를 적용해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49223295 0.32716955 0.31248978 0.44039751 0.63419865 0.18901812\n",
      " 0.43407316 0.52471111 0.70471633 0.51994118 0.38539967 0.85354351\n",
      " 0.48596947 0.19093055 0.65917464 0.62308927 0.47959846 0.12806924\n",
      " 0.33638497 0.42487822 0.65985891 0.7504125  0.57294577 0.43759534\n",
      " 0.30350925 0.81580116 0.16075162 0.65078061 0.31079729 0.27761989\n",
      " 0.75922705 0.75480326 0.5252742  0.27524425 0.54493274 0.48045608\n",
      " 0.47656534 0.36407508 0.61946376 0.7870455  0.54340242 0.45051339\n",
      " 0.2561386  0.46095375 0.47286217 0.71356805 0.72625386 0.73668337\n",
      " 0.4465477  0.45474909]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 element가 0에서 1사이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 다음에 다시 Dense 레이어가 출현합니다. 출력 노드 개수만 다를 뿐 동일한 구조입니다. 그렇다면 MLP 레이어를 아래와 같이 함수로 구현할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22212681 -0.35169022 -0.14984464 -0.43280809  0.15096991  0.25269151\n",
      " -0.22425944  0.21135301 -0.05539886 -0.72912229]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 최종 출력인 a2에 softmax 함수를 적용해 봅시다. 그러면 모델의 출력은 입력 X가 10가지 숫자 중 하나일 확률의 형태로 가공됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0897051 , 0.07880404, 0.09642928, 0.07266402, 0.13027198,\n",
       "       0.14422088, 0.089514  , 0.13838055, 0.10598055, 0.05402961])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 손실함수 (Loss Functions)\n",
    "이렇게 비선형 활성화 함수를 가진 여러 개의 은닉층을 거친 다음 신호 정보들은 출력층으로 전달됩니다. 이때 우리가 원하는 정답과 전달된 신호 정보들 사이의 차이를 계산하고, 이 차이를 줄이기 위해 각 파라미터들을 조정하는 것이 딥러닝의 전체적인 학습 흐름입니다. 이 차이를 구하는 데 사용되는 함수를 손실함수(Loss function) 또는 비용함수(Cost function)라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 최종 출력인 softmax(a2)와 정답 라벨의 One-hot 인코딩의 분포가 유사해져야 하는데, 아직은 별로 유사하지 않은 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0897051  0.07880404 0.09642928 0.07266402 0.13027198 0.14422088\n",
      " 0.089514   0.13838055 0.10598055 0.05402961]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3685226051997432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 경사하강법(Gradient Descent)\n",
    "오차를 구했으니 이제 이 오차를 줄이는 것이 목표입니다. 경사하강법은 각 단계에서의 기울기를 구해서 해당 기울기가 가리키는 방향으로 이동하는 방법입니다. 파라미터 W의 변화에 따른 오차(Loss) L의 변화량을 구하고, 오차 기울기가 커지는 방향의 반대 방향으로 파라미터를 조정해 주면 됩니다. 단, 조정을 너무 많이 해주면 안 되기 때문에 적절한 step size 역할을 하는 learning rate가 필수적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01794102,  0.01576081,  0.01928586,  0.0145328 ,  0.0260544 ,\n",
       "        -0.17115582,  0.0179028 ,  0.02767611,  0.02119611,  0.01080592],\n",
       "       [-0.18070895,  0.01449348,  0.01722496,  0.01326215,  0.02433321,\n",
       "         0.03012848,  0.01957733,  0.02963069,  0.02376874,  0.00828991],\n",
       "       [ 0.01972147,  0.01262656,  0.01889932,  0.01487119, -0.17707447,\n",
       "         0.02803165,  0.0170188 ,  0.03099004,  0.02304671,  0.01186874],\n",
       "       [ 0.01613613, -0.18438831,  0.01719282,  0.01660857,  0.02283587,\n",
       "         0.02584796,  0.02195277,  0.03150889,  0.01825854,  0.01404674],\n",
       "       [ 0.01543481,  0.01730405,  0.01836948,  0.0131456 ,  0.02575992,\n",
       "         0.02752701,  0.02093456,  0.02692036,  0.0230488 , -0.1884446 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dy가 구해지면 다른 기울기들은 chain-rule로 쉽게 구해집니다. 같은 방식으로 우리가 학습해야 할 모든 파라미터 W1, b1, W2, b2에 대한 기울기를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08623543, -0.09471581,  0.06694874,  0.05322302, -0.03090314,\n",
       "        -0.04384431,  0.07230277,  0.10775709,  0.08051198, -0.12504491],\n",
       "       [-0.06747238, -0.09749561,  0.06541313,  0.05209392, -0.02298349,\n",
       "        -0.0746352 ,  0.07023055,  0.10485437,  0.07795305, -0.10795834],\n",
       "       [-0.06264016, -0.07377634,  0.05991725,  0.04734092, -0.01018496,\n",
       "        -0.07424605,  0.06426896,  0.09538273,  0.07160977, -0.1176721 ],\n",
       "       [-0.02429846, -0.06061787,  0.0297654 ,  0.02414195, -0.02136238,\n",
       "        -0.04902318,  0.03160348,  0.04820779,  0.03488492, -0.01330166],\n",
       "       [-0.03118433, -0.11181983,  0.03077902,  0.02578943, -0.00582099,\n",
       "        -0.00443672,  0.03456888,  0.05142532,  0.03605486, -0.02535563],\n",
       "       [-0.02471147, -0.05994235,  0.03909119,  0.03117068, -0.03169231,\n",
       "        -0.03454818,  0.04180468,  0.06268461,  0.04648851, -0.07034536],\n",
       "       [-0.03286045, -0.05817343,  0.04233023,  0.03378808, -0.06595679,\n",
       "        -0.02450301,  0.04473525,  0.06834222,  0.05062387, -0.05832596],\n",
       "       [-0.1083033 , -0.06781146,  0.06037385,  0.04795489, -0.05149711,\n",
       "        -0.05368797,  0.06414662,  0.09762315,  0.07295559, -0.06175424],\n",
       "       [-0.03790607, -0.11723734,  0.06355399,  0.05115631, -0.06142175,\n",
       "        -0.05748698,  0.06795318,  0.10263808,  0.07518553, -0.08643495],\n",
       "       [-0.06844945, -0.05018935,  0.04960272,  0.03922452, -0.05390718,\n",
       "        -0.03487383,  0.05258186,  0.07979027,  0.05991269, -0.07369224],\n",
       "       [-0.10780831, -0.06049989,  0.05243304,  0.04154606, -0.02101957,\n",
       "        -0.03062334,  0.05657684,  0.08481502,  0.06378064, -0.0792005 ],\n",
       "       [-0.04439542, -0.0349158 ,  0.03763203,  0.02959556, -0.03055722,\n",
       "        -0.02760439,  0.04007017,  0.06010076,  0.04543429, -0.07535998],\n",
       "       [-0.10126089, -0.03105148,  0.0476846 ,  0.0374584 , -0.06971178,\n",
       "         0.01234529,  0.05084807,  0.07737669,  0.05889085, -0.08257975],\n",
       "       [-0.00098337, -0.02553702,  0.02820364,  0.02228002, -0.06613881,\n",
       "        -0.02015359,  0.02908933,  0.04499284,  0.03360471, -0.04535774],\n",
       "       [-0.07743326, -0.03476899,  0.05789782,  0.04528314, -0.08392438,\n",
       "        -0.00917775,  0.06126541,  0.09276032,  0.07070447, -0.12260678],\n",
       "       [-0.07871336, -0.01505691,  0.0332879 ,  0.02600098, -0.03460074,\n",
       "        -0.003956  ,  0.03547038,  0.05372188,  0.04115882, -0.05731295],\n",
       "       [-0.09710175, -0.0344954 ,  0.04595371,  0.03624189, -0.04387712,\n",
       "        -0.04405468,  0.04843903,  0.07415435,  0.05591154, -0.04117158],\n",
       "       [-0.01758371, -0.04320688,  0.0197075 ,  0.0161071 , -0.05325339,\n",
       "         0.02349133,  0.0212223 ,  0.03284505,  0.02378261, -0.0231119 ],\n",
       "       [-0.0977528 , -0.04299592,  0.06322001,  0.04977176, -0.08742616,\n",
       "        -0.06346083,  0.06594962,  0.10150068,  0.07649006, -0.06529642],\n",
       "       [-0.02637982, -0.11542679,  0.05305685,  0.04303556, -0.04593833,\n",
       "        -0.0507538 ,  0.05700627,  0.08602965,  0.06239525, -0.06302486],\n",
       "       [-0.02507419, -0.06199348,  0.02519925,  0.02051592, -0.01192703,\n",
       "        -0.00192603,  0.02782486,  0.04127848,  0.0300052 , -0.04390298],\n",
       "       [-0.08498061, -0.11026924,  0.05073646,  0.04098167, -0.00082659,\n",
       "         0.00550174,  0.05653362,  0.08307901,  0.06122543, -0.10198149],\n",
       "       [-0.08931746, -0.10808677,  0.06214359,  0.04970697,  0.00578663,\n",
       "        -0.05233648,  0.06788294,  0.10028297,  0.07442253, -0.11048493],\n",
       "       [-0.05224663, -0.10934615,  0.04305849,  0.03515727, -0.01586739,\n",
       "        -0.00671763,  0.04760911,  0.07076716,  0.05127807, -0.0636923 ],\n",
       "       [-0.05662876, -0.02129405,  0.0351155 ,  0.02770197, -0.05113266,\n",
       "        -0.06021796,  0.03603136,  0.05630914,  0.04218027, -0.00806483],\n",
       "       [-0.06974225, -0.03108824,  0.04106364,  0.03234682, -0.05410926,\n",
       "        -0.02292511,  0.04330257,  0.06618267,  0.04997504, -0.05500587],\n",
       "       [-0.02112766, -0.05116412,  0.03999463,  0.031798  , -0.07394881,\n",
       "        -0.00280905,  0.04238387,  0.06448067,  0.04805375, -0.07766129],\n",
       "       [-0.01353023, -0.05261457,  0.01633806,  0.01345592,  0.00245995,\n",
       "         0.00955313,  0.01864012,  0.02696826,  0.01940612, -0.04067675],\n",
       "       [ 0.00056005, -0.08981615,  0.02831386,  0.02338396, -0.0265629 ,\n",
       "         0.01157292,  0.03151247,  0.04663231,  0.03323798, -0.05883452],\n",
       "       [-0.09643779, -0.08780839,  0.0587932 ,  0.04723584, -0.05974716,\n",
       "        -0.06765494,  0.06220403,  0.09565682,  0.07033866, -0.02258028],\n",
       "       [-0.06537416, -0.05132302,  0.04463612,  0.03546454, -0.03337561,\n",
       "        -0.06385567,  0.04706444,  0.0717361 ,  0.05341608, -0.03838882],\n",
       "       [-0.0118316 , -0.06346634,  0.02971503,  0.02408041, -0.04184463,\n",
       "        -0.0043263 ,  0.03204491,  0.04841112,  0.03524441, -0.04802703],\n",
       "       [-0.03450804, -0.07720893,  0.04518864,  0.03632028, -0.04552062,\n",
       "        -0.05025727,  0.04800397,  0.0729509 ,  0.05349923, -0.04846816],\n",
       "       [-0.10287291, -0.09951988,  0.06966876,  0.05572023, -0.0604463 ,\n",
       "        -0.06874607,  0.07413237,  0.11282892,  0.08349563, -0.06426075],\n",
       "       [-0.02210798, -0.04028582,  0.03110704,  0.02478122, -0.03408859,\n",
       "        -0.04805956,  0.03255678,  0.04977607,  0.03681601, -0.03049519],\n",
       "       [-0.10467397, -0.03150226,  0.03394192,  0.02673386,  0.0039956 ,\n",
       "        -0.00480957,  0.03720604,  0.0552047 ,  0.04205955, -0.05815587],\n",
       "       [-0.06111569, -0.07896799,  0.06236674,  0.04959755, -0.08618328,\n",
       "        -0.02704116,  0.06625142,  0.10061503,  0.07499689, -0.10051951],\n",
       "       [-0.07450443, -0.03085748,  0.03291195,  0.02601281, -0.01685369,\n",
       "        -0.02885221,  0.03514367,  0.05319163,  0.04007466, -0.03626692],\n",
       "       [-0.0372126 , -0.05669576,  0.03006307,  0.02437026, -0.05822192,\n",
       "         0.00937997,  0.03223302,  0.04959496,  0.03621528, -0.02972626],\n",
       "       [-0.03871962, -0.02421107,  0.03038284,  0.02400137, -0.0535042 ,\n",
       "        -0.02271489,  0.03164437,  0.04891557,  0.03669651, -0.03249088],\n",
       "       [-0.04773317, -0.00613799,  0.02863344,  0.02229605, -0.04818444,\n",
       "        -0.02016258,  0.02969026,  0.0457903 ,  0.03500376, -0.03919563],\n",
       "       [-0.06896554, -0.04280212,  0.03444423,  0.02744383, -0.02949221,\n",
       "        -0.01953722,  0.0368634 ,  0.05602158,  0.04179553, -0.0357715 ],\n",
       "       [-0.10513924, -0.02606619,  0.04366073,  0.03438457, -0.0595563 ,\n",
       "        -0.01978905,  0.04597997,  0.07088068,  0.05364921, -0.03800439],\n",
       "       [-0.06096366, -0.06343703,  0.05547074,  0.04370392, -0.01738965,\n",
       "        -0.04459916,  0.05975284,  0.0884196 ,  0.06674164, -0.12769925],\n",
       "       [-0.05414802, -0.11035679,  0.04508264,  0.03688178, -0.03244041,\n",
       "        -0.02630142,  0.04904927,  0.07413961,  0.05342542, -0.03533209],\n",
       "       [-0.0733389 , -0.05861655,  0.05050326,  0.03989449,  0.01063992,\n",
       "        -0.08906366,  0.05400314,  0.08035031,  0.06030166, -0.07467366],\n",
       "       [-0.09212307, -0.02684739,  0.05351108,  0.04161617, -0.02214983,\n",
       "        -0.04430643,  0.05705823,  0.08503044,  0.0652767 , -0.1170659 ],\n",
       "       [-0.0365528 , -0.02717976,  0.03325893,  0.02593592,  0.0029796 ,\n",
       "        -0.0406966 ,  0.03571393,  0.05241528,  0.04002503, -0.08589952],\n",
       "       [-0.11501752, -0.05522482,  0.05017936,  0.03972623, -0.0357401 ,\n",
       "         0.00832952,  0.05452012,  0.08171459,  0.06172405, -0.09021142],\n",
       "       [-0.00110113, -0.07966104,  0.01676588,  0.01444227, -0.01729927,\n",
       "         0.00364708,  0.01887272,  0.02849485,  0.01921123, -0.00337258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간에 sigmoid가 한번 사용되었으므로, 활성화함수에 대한 gradient도 고려되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터를 업데이트하는 함수에서는 learning_rate도 고려해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 오차역전파법(Backpropagation)\n",
    "오차역전파법은 출력층의 결과와 내가 뽑고자 하는 target 값과의 차이를 구한 뒤, 그 오차값을 각 레이어들을 지나며 역전파해가며 각 노드가 가지고 있는 변수들을 갱신해 나가는 방식입니다. 이 과정을 하나의 레이어에 대해 다음과 같이 정리해 볼 수 있습니다. 이전의 `affine_layer_forward(X, W, b)`에 대응하여 생각해 보면 해당 레이어의 backpropagation 함수를 얻을 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상 정리된 내용을 바탕으로 Forward Propagation과 Backward Propagation이 이루어지는 한 사이클을 아래와 같이 엮어 볼 수 있을 것입니다. 한 스텝의 Forward Propagation과 Backward Propagation을 통해 학습해야 할 파라미터 W1, b1, W2, b2가 업데이트되는 과정을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11527186 0.05521085 0.13979357 0.07589706 0.07277085 0.08325258\n",
      "  0.07623903 0.1725616  0.13175446 0.07724814]\n",
      " [0.10108792 0.07345715 0.12181273 0.06985792 0.07886796 0.10778283\n",
      "  0.08346225 0.16982749 0.11981147 0.07403228]\n",
      " [0.11214863 0.06693876 0.169342   0.09228219 0.08146217 0.0874699\n",
      "  0.0705663  0.14922832 0.10143324 0.06912849]\n",
      " [0.1111217  0.07538064 0.16439356 0.09321103 0.06900729 0.10196346\n",
      "  0.07397186 0.1519717  0.09692837 0.0620504 ]\n",
      " [0.10789534 0.05997231 0.17808889 0.08876315 0.07209836 0.08857167\n",
      "  0.08029253 0.14383748 0.11984338 0.06063689]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.534662785809496\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_ont_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 모델 학습 Step-by-Step\n",
    "업데이트되는 과정을 다섯 스텝 반복해 보면서 모델이 정확한 추론에 근접해 가는지 확인해 보도록 하겠습니다. 모델이 추론한 확률값 y_hat이 정답의 One-hot 인코딩 t값에 조금씩 근접하는 것과, Loss가 점점 감소하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_ont_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.12930131 0.03156767 0.09487168 0.12056043 0.10786856 0.07804774\n",
      "  0.12773016 0.08351543 0.15776034 0.06877667]\n",
      " [0.12880461 0.03958225 0.10171906 0.12858376 0.10370796 0.10616023\n",
      "  0.10632311 0.06828781 0.1539637  0.06286752]\n",
      " [0.12762736 0.04368263 0.08360212 0.11611605 0.10621414 0.10727611\n",
      "  0.10712119 0.08152793 0.16090351 0.06592894]\n",
      " [0.08035931 0.04520745 0.08914263 0.12477122 0.09334449 0.12449381\n",
      "  0.09148712 0.09155414 0.18346666 0.07617316]\n",
      " [0.10846584 0.05161302 0.10227242 0.12113962 0.09651624 0.10646142\n",
      "  0.10936584 0.09134667 0.13516221 0.07765673]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.49882836712021\n",
      "---------\n",
      "[[0.15276473 0.0403987  0.08455583 0.10294193 0.12474028 0.09787757\n",
      "  0.11112136 0.0752234  0.12718763 0.08318857]\n",
      " [0.15663391 0.04953972 0.08976568 0.10864718 0.11874678 0.12647262\n",
      "  0.0918738  0.06105781 0.12221207 0.07505043]\n",
      " [0.14584532 0.0531662  0.07450977 0.09971691 0.12500322 0.12457494\n",
      "  0.09367469 0.07398729 0.132934   0.07658764]\n",
      " [0.09199867 0.05806707 0.08057292 0.10863898 0.10585128 0.14696451\n",
      "  0.08118564 0.0836857  0.15230796 0.09072728]\n",
      " [0.12299564 0.06356156 0.09165837 0.10480851 0.1081144  0.12364437\n",
      "  0.09639711 0.08311753 0.11160102 0.09410149]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.2933671100814395\n",
      "---------\n",
      "[[0.17082799 0.04944013 0.07456442 0.0880236  0.13743076 0.11619614\n",
      "  0.09619642 0.06679992 0.10457216 0.09594845]\n",
      " [0.18045067 0.05923787 0.07841211 0.09197468 0.12970785 0.14282121\n",
      "  0.07903526 0.05386633 0.09901691 0.0854771 ]\n",
      " [0.15948563 0.0624565  0.06600543 0.08598038 0.14155424 0.13846187\n",
      "  0.08180288 0.06655888 0.1120324  0.08566178]\n",
      " [0.1004612  0.07172244 0.07227351 0.09487172 0.11510808 0.16546901\n",
      "  0.07184068 0.07568239 0.12885758 0.10371339]\n",
      " [0.13358776 0.07551064 0.08172757 0.09114891 0.11656763 0.13750938\n",
      "  0.08493992 0.07505334 0.09414049 0.10981435]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.132752280385559\n",
      "---------\n",
      "[[0.18398488 0.05855007 0.06563527 0.07573156 0.14654527 0.13263366\n",
      "  0.08345449 0.0590566  0.08742539 0.10698282]\n",
      " [0.20047996 0.06854339 0.06842707 0.07841258 0.13734116 0.15536978\n",
      "  0.06818913 0.04736058 0.0816631  0.09421326]\n",
      " [0.16922213 0.0714608  0.0585131  0.07469334 0.156195   0.14915419\n",
      "  0.07172024 0.05979723 0.0960105  0.09323348]\n",
      " [0.10619262 0.08605099 0.06478301 0.0833806  0.12165777 0.18009307\n",
      "  0.06373658 0.06824638 0.11074199 0.115117  ]\n",
      " [0.1408533  0.08733754 0.07292745 0.07987157 0.12244438 0.1482344\n",
      "  0.07514535 0.06769683 0.08076986 0.12471931]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.0036720564428263\n",
      "---------\n",
      "[[0.19318424 0.06758873 0.05792892 0.06567948 0.15286021 0.14719225\n",
      "  0.07279389 0.05224862 0.07415916 0.11636451]\n",
      " [0.21741943 0.07733024 0.05992742 0.06745932 0.14246848 0.1646524\n",
      "  0.05919988 0.04171493 0.06841223 0.10141567]\n",
      " [0.17590967 0.08007057 0.05207309 0.06544422 0.16937461 0.15710704\n",
      "  0.06327105 0.05385097 0.08346398 0.0994348 ]\n",
      " [0.10977553 0.10090865 0.05821813 0.07382949 0.12609736 0.19128416\n",
      "  0.05682213 0.06160386 0.09644057 0.12502012]\n",
      " [0.14553174 0.09890476 0.06530155 0.07055892 0.12633264 0.15623424\n",
      "  0.06686123 0.06119284 0.07026621 0.13881586]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.8971463179937225\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 추론 과정 구현과 정확도(Accuracy) 계산\n",
    "방금 5스텝 학습한 파라미터 W1, b1, W2, b2를 가지고 숫자를 인식(Predict)해보고, 추론한 결과의 정확도(Accuracy)를 측정해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19940338, 0.07639822, 0.05136963, 0.05744313, 0.15708193,\n",
       "       0.16007265, 0.06392373, 0.04637885, 0.06370757, 0.12422091])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "\n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "   # t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19940338 0.07639822 0.05136963 0.05744313 0.15708193 0.16007265\n",
      " 0.06392373 0.04637885 0.06370757 0.12422091]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_ont_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5스텝만 가지고는 아직 10% 정도의 정확도에도 미치지 못하고 있습니다. 그럼 전체 학습 사이클을 제대로 수행해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 전체 학습 사이클 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킬 파라미터를 초기화하는 함수\n",
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n",
      "Loss:  2.295821567833413\n",
      "train acc, test acc | 0.10441666666666667, 0.1028\n",
      "Loss:  0.808241567966425\n",
      "train acc, test acc | 0.8019666666666667, 0.808\n",
      "Loss:  0.4775381153836402\n",
      "train acc, test acc | 0.8775333333333334, 0.8813\n",
      "Loss:  0.3352083658683854\n",
      "train acc, test acc | 0.8980666666666667, 0.9019\n",
      "Loss:  0.33304739733472455\n",
      "train acc, test acc | 0.9085333333333333, 0.9114\n",
      "Loss:  0.31881349318945423\n",
      "train acc, test acc | 0.9151666666666667, 0.9174\n",
      "Loss:  0.3578052027901717\n",
      "train acc, test acc | 0.9198833333333334, 0.9229\n",
      "Loss:  0.1959593771817197\n",
      "train acc, test acc | 0.92475, 0.9256\n",
      "Loss:  0.34603414119528\n",
      "train acc, test acc | 0.9282333333333334, 0.9292\n",
      "Loss:  0.27635073693504647\n",
      "train acc, test acc | 0.9312666666666667, 0.9306\n",
      "Loss:  0.38885985763788516\n",
      "train acc, test acc | 0.9339, 0.9338\n",
      "Loss:  0.17400292954415494\n",
      "train acc, test acc | 0.9368833333333333, 0.9358\n",
      "Loss:  0.17294633268222592\n",
      "train acc, test acc | 0.9396666666666667, 0.9373\n",
      "Loss:  0.30273447918620905\n",
      "train acc, test acc | 0.9411, 0.9375\n",
      "Loss:  0.30950382179057334\n",
      "train acc, test acc | 0.9436333333333333, 0.9402\n",
      "Loss:  0.2403277694053191\n",
      "train acc, test acc | 0.9461333333333334, 0.9428\n",
      "Loss:  0.08209776240165324\n",
      "train acc, test acc | 0.9476166666666667, 0.945\n",
      "Loss:  0.22051406784958708\n",
      "train acc, test acc | 0.9493833333333334, 0.9454\n",
      "Loss:  0.24925163728353092\n",
      "train acc, test acc | 0.95045, 0.9473\n",
      "Loss:  0.1503230856166001\n",
      "train acc, test acc | 0.9521333333333334, 0.9481\n",
      "Loss:  0.13167580558303774\n",
      "train acc, test acc | 0.9537666666666667, 0.9492\n",
      "Loss:  0.1253161803695464\n",
      "train acc, test acc | 0.9543666666666667, 0.9491\n",
      "Loss:  0.1628678076149637\n",
      "train acc, test acc | 0.9561833333333334, 0.9513\n",
      "Loss:  0.15247364200027755\n",
      "train acc, test acc | 0.9567166666666667, 0.9517\n",
      "Loss:  0.17467113256753666\n",
      "train acc, test acc | 0.9583333333333334, 0.9526\n",
      "Loss:  0.1080135142128301\n",
      "train acc, test acc | 0.9590833333333333, 0.9536\n",
      "Loss:  0.12133648268239237\n",
      "train acc, test acc | 0.9600333333333333, 0.9546\n",
      "Loss:  0.10739220596897876\n",
      "train acc, test acc | 0.9610833333333333, 0.9545\n",
      "Loss:  0.16367409029701302\n",
      "train acc, test acc | 0.9622333333333334, 0.9568\n",
      "Loss:  0.06988456988430776\n",
      "train acc, test acc | 0.9624666666666667, 0.9565\n",
      "Loss:  0.16408474910556906\n",
      "train acc, test acc | 0.9638833333333333, 0.9577\n",
      "Loss:  0.051621991649850735\n",
      "train acc, test acc | 0.9641166666666666, 0.9579\n",
      "Loss:  0.05570024552405153\n",
      "train acc, test acc | 0.9651, 0.9582\n",
      "Loss:  0.10151747507691056\n",
      "train acc, test acc | 0.9659833333333333, 0.9598\n",
      "Loss:  0.09636287322283298\n",
      "train acc, test acc | 0.96645, 0.9607\n",
      "Loss:  0.08344968414251086\n",
      "train acc, test acc | 0.9673, 0.9608\n",
      "Loss:  0.12752779666766242\n",
      "train acc, test acc | 0.9676333333333333, 0.9608\n",
      "Loss:  0.08177728756507573\n",
      "train acc, test acc | 0.9684833333333334, 0.9621\n",
      "Loss:  0.15478238327043886\n",
      "train acc, test acc | 0.9692, 0.9619\n",
      "Loss:  0.08269799836316997\n",
      "train acc, test acc | 0.9695666666666667, 0.9619\n",
      "Loss:  0.06734928931678867\n",
      "train acc, test acc | 0.9699333333333333, 0.9636\n",
      "Loss:  0.09519375589541729\n",
      "train acc, test acc | 0.9705833333333334, 0.9626\n",
      "Loss:  0.11109922540193473\n",
      "train acc, test acc | 0.9710666666666666, 0.9635\n",
      "Loss:  0.12781015272358956\n",
      "train acc, test acc | 0.97175, 0.9644\n",
      "Loss:  0.16227563791665323\n",
      "train acc, test acc | 0.9721333333333333, 0.9641\n",
      "Loss:  0.07319829767693287\n",
      "train acc, test acc | 0.9725333333333334, 0.9645\n",
      "Loss:  0.12152527259136224\n",
      "train acc, test acc | 0.9729333333333333, 0.9649\n",
      "Loss:  0.10692509636783873\n",
      "train acc, test acc | 0.9735833333333334, 0.9648\n",
      "Loss:  0.05100979854432443\n",
      "train acc, test acc | 0.9739, 0.9651\n",
      "Loss:  0.10367500987177572\n",
      "train acc, test acc | 0.9739833333333333, 0.9654\n",
      "Loss:  0.043554649026450225\n",
      "train acc, test acc | 0.9746833333333333, 0.9664\n",
      "Loss:  0.12491675393453054\n",
      "train acc, test acc | 0.975, 0.9663\n",
      "Loss:  0.08718255647634102\n",
      "train acc, test acc | 0.9752333333333333, 0.9668\n",
      "Loss:  0.1306242790391209\n",
      "train acc, test acc | 0.9757666666666667, 0.9672\n",
      "Loss:  0.0771393720159511\n",
      "train acc, test acc | 0.9761166666666666, 0.9665\n",
      "Loss:  0.07500986194043983\n",
      "train acc, test acc | 0.9763833333333334, 0.968\n",
      "Loss:  0.10196687527938669\n",
      "train acc, test acc | 0.9768833333333333, 0.9667\n",
      "Loss:  0.04141086560144985\n",
      "train acc, test acc | 0.977, 0.9686\n",
      "Loss:  0.11643652403096352\n",
      "train acc, test acc | 0.9772166666666666, 0.9689\n",
      "Loss:  0.07132731674115632\n",
      "train acc, test acc | 0.9775166666666667, 0.9681\n",
      "Loss:  0.035674682664029726\n",
      "train acc, test acc | 0.9776833333333333, 0.9679\n",
      "Loss:  0.04411550918968442\n",
      "train acc, test acc | 0.9779833333333333, 0.9683\n",
      "Loss:  0.04262032187960936\n",
      "train acc, test acc | 0.9785833333333334, 0.9685\n",
      "Loss:  0.0533045825631693\n",
      "train acc, test acc | 0.97855, 0.9687\n",
      "Loss:  0.042564124784796535\n",
      "train acc, test acc | 0.9790333333333333, 0.9687\n",
      "Loss:  0.024853222583030514\n",
      "train acc, test acc | 0.97935, 0.9693\n",
      "Loss:  0.06591420708399283\n",
      "train acc, test acc | 0.97965, 0.9692\n",
      "Loss:  0.060231918011421554\n",
      "train acc, test acc | 0.9794666666666667, 0.9696\n",
      "Loss:  0.07601078493224907\n",
      "train acc, test acc | 0.9797666666666667, 0.9693\n",
      "Loss:  0.11864299339899265\n",
      "train acc, test acc | 0.9796166666666667, 0.9684\n",
      "Loss:  0.10845068306881193\n",
      "train acc, test acc | 0.9801166666666666, 0.9694\n",
      "Loss:  0.04839649322688915\n",
      "train acc, test acc | 0.9807333333333333, 0.9691\n",
      "Loss:  0.03112632548609651\n",
      "train acc, test acc | 0.981, 0.9692\n",
      "Loss:  0.045168819435203886\n",
      "train acc, test acc | 0.9811666666666666, 0.9705\n",
      "Loss:  0.03868351148167612\n",
      "train acc, test acc | 0.9813833333333334, 0.9698\n",
      "Loss:  0.04022291858845293\n",
      "train acc, test acc | 0.9816, 0.9702\n",
      "Loss:  0.055887517710635236\n",
      "train acc, test acc | 0.9816666666666667, 0.9713\n",
      "Loss:  0.03840277622855155\n",
      "train acc, test acc | 0.9821666666666666, 0.9709\n",
      "Loss:  0.09538209025604046\n",
      "train acc, test acc | 0.9822166666666666, 0.9714\n",
      "Loss:  0.06465613750095073\n",
      "train acc, test acc | 0.9825, 0.9708\n",
      "Loss:  0.08948469761337863\n",
      "train acc, test acc | 0.9825, 0.9709\n",
      "Loss:  0.053131069274029316\n",
      "train acc, test acc | 0.9826333333333334, 0.9705\n",
      "Loss:  0.13242316283769873\n",
      "train acc, test acc | 0.98285, 0.9712\n",
      "Loss:  0.045517280460362804\n",
      "train acc, test acc | 0.9828166666666667, 0.9712\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
