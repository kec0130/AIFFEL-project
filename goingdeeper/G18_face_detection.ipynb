{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#데이터셋-전처리\" data-toc-modified-id=\"데이터셋-전처리-1\">데이터셋 전처리</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1)-데이터셋-분석\" data-toc-modified-id=\"(1)-데이터셋-분석-1.1\">(1) 데이터셋 분석</a></span></li><li><span><a href=\"#(2)-tf_example-생성\" data-toc-modified-id=\"(2)-tf_example-생성-1.2\">(2) tf_example 생성</a></span></li></ul></li><li><span><a href=\"#모델-구현\" data-toc-modified-id=\"모델-구현-2\">모델 구현</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1)-prior-box\" data-toc-modified-id=\"(1)-prior-box-2.1\">(1) prior box</a></span></li><li><span><a href=\"#(2)-SSD-model\" data-toc-modified-id=\"(2)-SSD-model-2.2\">(2) SSD model</a></span></li></ul></li><li><span><a href=\"#모델-학습\" data-toc-modified-id=\"모델-학습-3\">모델 학습</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1)-Augmentation,-Prior-적용\" data-toc-modified-id=\"(1)-Augmentation,-Prior-적용-3.1\">(1) Augmentation, Prior 적용</a></span><ul class=\"toc-item\"><li><span><a href=\"#Augmentation\" data-toc-modified-id=\"Augmentation-3.1.1\">Augmentation</a></span></li><li><span><a href=\"#Prior-box\" data-toc-modified-id=\"Prior-box-3.1.2\">Prior box</a></span></li><li><span><a href=\"#load_dataset\" data-toc-modified-id=\"load_dataset-3.1.3\">load_dataset</a></span></li></ul></li><li><span><a href=\"#(2)-Train\" data-toc-modified-id=\"(2)-Train-3.2\">(2) Train</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-rate-scheduler\" data-toc-modified-id=\"Learning-rate-scheduler-3.2.1\">Learning rate scheduler</a></span></li><li><span><a href=\"#Hard-negative-mining\" data-toc-modified-id=\"Hard-negative-mining-3.2.2\">Hard negative mining</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3.2.3\">Training</a></span></li></ul></li></ul></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-4\">Inference</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1)-NMS\" data-toc-modified-id=\"(1)-NMS-4.1\">(1) NMS</a></span></li><li><span><a href=\"#(2)-사진에서-얼굴-찾기\" data-toc-modified-id=\"(2)-사진에서-얼굴-찾기-4.2\">(2) 사진에서 얼굴 찾기</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 전처리\n",
    "## (1) 데이터셋 분석\n",
    "- WIDER FACE 데이터셋은 Face detection을 위한 데이터셋이므로 입력데이터는 이미지 파일, Ground Truth는 Bounding box 정보로 되어 있습니다.\n",
    "- `wider_face_train_bbx_gt.txt`에는 boudbing box 정보가 [x, y, w, h] 형태로 저장되어 있습니다. \n",
    "- bounding box 정보를 파싱해서 리스트로 추출하고, x_min, y_min, x_max, y_max 형태의 꼭지점 좌표 정보로 변환하여 출력해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'filename': '/home/ssac21/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac21/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac21/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac21/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac21/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) tf_example 생성\n",
    "- 대용량 데이터셋의 처리속도 향상을 위해서 전처리 작업을 통해 tfrecord 데이터셋으로 변환합니다. 1개 데이터의 단위를 이루는 `tf.train.Example` 인스턴스를 생성하는 메소드는 아래와 같습니다.\n",
    "- 데이터셋의 이미지파일, 그리고 bounding box를 파싱한 정보를 모아 `make_example` 메소드를 통해 만든 example을 serialize하여 바이너리 파일로 생성하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12880/12880 [00:42<00:00, 304.09it/s]\n",
      "100%|██████████| 3226/3226 [00:10<00:00, 315.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현\n",
    "## (1) prior box\n",
    "- SSD 모델의 가장 중요한 특징 중 하나는 prior box(또는 anchor box)를 필요로 한다는 점입니다.\n",
    "- prior box란, object가 존재할 만한 다양한 크기의 box의 좌표 및 클래스 정보를 일정 개수만큼 미리 고정해 둔 것입니다.\n",
    "- ground truth에 해당하는 bounding box와의 IoU를 계산하여 일정 이상(0.5) 겹치는 prior box를 선택하는 방식이 RCNN 계열의 sliding window 방식보다 훨씬 속도가 빠르면서도 그와 유사한 정도의 정확도를 얻을 수 있다는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 100,\n",
       " 'init_lr': 0.01,\n",
       " 'lr_decay_epoch': [50, 70],\n",
       " 'lr_rate': 0.1,\n",
       " 'warmup_epoch': 5,\n",
       " 'min_lr': 0.0001,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.5,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config 정보를 모아 dict 구조로 정리합니다.\n",
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,  # if False,training from scratch\n",
    "    \"epoch\": 100,\n",
    "    \"init_lr\": 1e-2,\n",
    "    \"lr_decay_epoch\": [50, 70],\n",
    "    \"lr_rate\": 0.1,\n",
    "    \"warmup_epoch\": 5,\n",
    "    \"min_lr\": 1e-4,\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "# config 중 prior(anchor) box 생성과 관련된 것들은 다음과 같습니다.\n",
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior box를 생성하기 위해 기준이 되는 feature map을 먼저 생성합니다.\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature map별로 순회를 하면서 prior box 를 생성합니다.\n",
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior box를 생성하는 최종 메소드\n",
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)      (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, None, None, 1 432         conv_pad_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_1 (BatchNormalization)  (None, None, None, 1 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_1 (ReLU)              (None, None, None, 1 0           conv_bn_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, None, None, 3 4608        conv_relu_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_2 (BatchNormalization)  (None, None, None, 3 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_2 (ReLU)              (None, None, None, 3 0           conv_bn_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, None, None, 3 9216        conv_pad_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_3 (BatchNormalization)  (None, None, None, 3 128         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_3 (ReLU)              (None, None, None, 3 0           conv_bn_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, None, None, 3 9216        conv_relu_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_4 (BatchNormalization)  (None, None, None, 3 128         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_4 (ReLU)              (None, None, None, 3 0           conv_bn_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, None, None, 6 18432       conv_pad_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_5 (BatchNormalization)  (None, None, None, 6 256         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_5 (ReLU)              (None, None, None, 6 0           conv_bn_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, None, None, 6 36864       conv_relu_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_6 (BatchNormalization)  (None, None, None, 6 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_6 (ReLU)              (None, None, None, 6 0           conv_bn_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, None, None, 6 36864       conv_relu_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_7 (BatchNormalization)  (None, None, None, 6 256         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_7 (ReLU)              (None, None, None, 6 0           conv_bn_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, None, None, 6 36864       conv_relu_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_8 (BatchNormalization)  (None, None, None, 6 256         conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_8 (ReLU)              (None, None, None, 6 0           conv_bn_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)      (None, None, None, 6 0           conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, None, None, 1 73728       conv_pad_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_9 (BatchNormalization)  (None, None, None, 1 512         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_9 (ReLU)              (None, None, None, 1 0           conv_bn_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, None, None, 1 147456      conv_relu_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_10 (BatchNormalization) (None, None, None, 1 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_10 (ReLU)             (None, None, None, 1 0           conv_bn_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, None, None, 1 147456      conv_relu_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_11 (BatchNormalization) (None, None, None, 1 512         conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_11 (ReLU)             (None, None, None, 1 0           conv_bn_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)          (None, None, None, 1 0           conv_dw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)             (None, None, None, 2 32768       conv_dw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)          (None, None, None, 2 0           conv_pw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)          (None, None, None, 2 0           conv_dw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)             (None, None, None, 2 65536       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)          (None, None, None, 2 0           conv_pw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_14 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_relu (ReLU)          (None, None, None, 2 0           conv_dw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14 (Conv2D)             (None, None, None, 2 65536       conv_dw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_relu (ReLU)          (None, None, None, 2 0           conv_pw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 9232        conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 18448       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 36880       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 36880       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 1 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 1 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 18464       conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 3 36896       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 3 73760       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 73760       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 4 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 4 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 4 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, None, 4 0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, None, 4 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, None, 4 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, None, 4 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 5196        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 8 3464        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 8 3464        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 5196        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 2598        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 1732        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 4 1732        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 2598        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 4)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, None, 4)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, None, 4)      0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, None, 4)      0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, None, 2)      0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 2)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, None, 2)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, None, 2)      0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습\n",
    "## (1) Augmentation, Prior 적용\n",
    "### Augmentation\n",
    "아래는 augmentation을 위해 `tf.data.TFRecordDataset.map()` 내에서 호출할 메소드들입니다.\n",
    "- `_crop`\n",
    "- `_pad_to_square`\n",
    "- `_resize`\n",
    "- `_flip`\n",
    "- `_distort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior box\n",
    "- prior box와 bounding box 사이의 IoU, 다른 말로 jaccard index를 계산한다.\n",
    "- jaccard 메소드를 이용해 label의 ground truth bbox와 가장 overlap 비율이 높은 matched prior를 구한다.\n",
    "- `_encode_bbox` 메소드를 통해 bbox의 scale을 동일하게 보정한다.\n",
    "- 전체 prior box에 대해 일정 threshold 이상 overlap되는 ground truth bounding box 존재 여부(positive/negative)를 concat하여 새로운 label로 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_dataset\n",
    "위에서 구현한 두가지 메소드를 이전 스텝에서 생성한 tfrecord 데이터셋에 적용하여 SSD 학습을 위한 데이터셋을 생성하는 최종 메소드인 `load_dataset` 을 구현합니다.\n",
    "- `_transform_data` : aumemtation과 prior box label을 적용하여 기존의 dataset을 변환하는 메소드\n",
    "- `_parse_tfrecord` : tfrecord 에 `_transform_data`를 적용하는 함수 클로저 생성\n",
    "- `load_tfrecord_dataset` : `tf.data.TFRecordDataset.map()`에 `_parse_tfrecord`을 적용하는 실제 데이터셋 변환 메인 메소드\n",
    "- `load_dataset` : `load_tfrecord_dataset`을 통해 train, validation 데이터셋을 생성하는 최종 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=True,\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train\n",
    "### Learning rate scheduler\n",
    "구간별로 learning rate가 일정하게 유지하면서 감소하는 PiecewiseConstantDecay를 상속받아, 초기시점에 WarmUp부분을 추가한 `PiecewiseConstantWarmUpDecay`를 활용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard negative mining\n",
    "- Object Detection 모델 학습시 자주 사용되는 Hard negative mining이라는 기법이 있습니다. 학습과정에서 label은 negative인데 confidence가 높게 나오는 샘플을 재학습하면 positive와 negative의 모호한 경계선상에 분포한 false negative 오류에 강해진다는 장점이 있습니다.\n",
    "\n",
    "- 실제로 confidence가 높은 샘플을 모아 training을 다시 수행하기보다는, 그런 샘플들에 대한 loss만 따로 모아 계산해주는 방식으로 반영할 수 있습니다.\n",
    "\n",
    "- 아래 구현된 `hard_negative_mining` 메소드와, 이 메소드를 통해 얻은 샘플을 통해 얻은 localization loss를 기존의 classification loss에 추가로 반영하는 MultiBoxLoss 계산 메소드를 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "배치사이즈, epoch 수 등 학습에 대한 기본설정은 cfg dict 내용을 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "global load_t1\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 240, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_15 (ZeroPadding2D)     (None, 242, 322, 3)  0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 120, 160, 16) 432         conv_pad_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_15 (BatchNormalization) (None, 120, 160, 16) 64          conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_15 (ReLU)             (None, 120, 160, 16) 0           conv_bn_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 120, 160, 32) 4608        conv_relu_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_16 (BatchNormalization) (None, 120, 160, 32) 128         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_16 (ReLU)             (None, 120, 160, 32) 0           conv_bn_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_17 (ZeroPadding2D)     (None, 122, 162, 32) 0           conv_relu_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 60, 80, 32)   9216        conv_pad_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_17 (BatchNormalization) (None, 60, 80, 32)   128         conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_17 (ReLU)             (None, 60, 80, 32)   0           conv_bn_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 60, 80, 32)   9216        conv_relu_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_18 (BatchNormalization) (None, 60, 80, 32)   128         conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_18 (ReLU)             (None, 60, 80, 32)   0           conv_bn_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_19 (ZeroPadding2D)     (None, 62, 82, 32)   0           conv_relu_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 30, 40, 64)   18432       conv_pad_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_19 (BatchNormalization) (None, 30, 40, 64)   256         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_19 (ReLU)             (None, 30, 40, 64)   0           conv_bn_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_20 (BatchNormalization) (None, 30, 40, 64)   256         conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_20 (ReLU)             (None, 30, 40, 64)   0           conv_bn_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_21 (BatchNormalization) (None, 30, 40, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_21 (ReLU)             (None, 30, 40, 64)   0           conv_bn_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_22 (BatchNormalization) (None, 30, 40, 64)   256         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_22 (ReLU)             (None, 30, 40, 64)   0           conv_bn_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_23 (ZeroPadding2D)     (None, 32, 42, 64)   0           conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 15, 20, 128)  73728       conv_pad_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_23 (BatchNormalization) (None, 15, 20, 128)  512         conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_23 (ReLU)             (None, 15, 20, 128)  0           conv_bn_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_24 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_24 (BatchNormalization) (None, 15, 20, 128)  512         conv_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_24 (ReLU)             (None, 15, 20, 128)  0           conv_bn_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_25 (BatchNormalization) (None, 15, 20, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_25 (ReLU)             (None, 15, 20, 128)  0           conv_bn_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_26 (ZeroPadding2D)     (None, 17, 22, 128)  0           conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_28 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 30, 40, 16)   9232        conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 15, 20, 16)   18448       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 30, 40, 16)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 15, 20, 16)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 8, 10, 16)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 4, 5, 16)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 30, 40, 16)   2320        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 30, 40, 32)   18464       conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 15, 20, 16)   2320        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 15, 20, 32)   36896       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 30, 40, 48)   0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15, 20, 48)   0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 10, 48)    0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 5, 48)     0           conv2d_30[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 30, 40, 48)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 15, 20, 48)   0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 8, 10, 48)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 4, 5, 48)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 30, 40, 12)   5196        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 15, 20, 8)    3464        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 30, 40, 6)    2598        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 15, 20, 4)    1732        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, None, 4)      0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, None, 4)      0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, None, 4)      0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, None, 4)      0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, None, 2)      0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, None, 2)      0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, None, 2)      0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, None, 2)      0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_9[0][0]                  \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape_8[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Create Model...\")\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=True)\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'),\n",
    "                              show_shapes=True, show_layer_names=True)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    logging.info(\"Create network failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['resume']:\n",
    "    # Training from latest weights\n",
    "    paths = [os.path.join(weights_dir, path)\n",
    "             for path in os.listdir(weights_dir)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    init_epoch = int(os.path.splitext(latest)[0][-3:])\n",
    "\n",
    "else:\n",
    "    init_epoch = -1\n",
    "\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "logging.info(f\"steps_per_epoch:{steps_per_epoch}\")\n",
    "\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=cfg['momentum'], nesterov=True)\n",
    "multi_loss = MultiBoxLoss(num_class=len(label_classes), neg_pos_ratio=3)\n",
    "train_log_dir = 'logs/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n",
    "        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Batch 402/402 | Batch time 0.137 || Loss: 6.849316 | loc loss:4.511279 | class loss:2.338037   \n",
      "Epoch: 1/100  | Epoch time 221.153 || Average Loss: inf\n",
      "Epoch: 2/100 | Batch 402/402 | Batch time 0.138 || Loss: 5.859300 | loc loss:3.038186 | class loss:2.821114  \n",
      "Epoch: 2/100  | Epoch time 56.705 || Average Loss: inf\n",
      "Epoch: 3/100 | Batch 402/402 | Batch time 0.136 || Loss: 6.149025 | loc loss:3.541139 | class loss:2.607887 \n",
      "Epoch: 3/100  | Epoch time 58.631 || Average Loss: inf\n",
      "Epoch: 4/100 | Batch 402/402 | Batch time 0.147 || Loss: 6.072237 | loc loss:3.957820 | class loss:2.114416  \n",
      "Epoch: 4/100  | Epoch time 58.943 || Average Loss: inf\n",
      "Epoch: 5/100 | Batch 402/402 | Batch time 0.153 || Loss: 5.844029 | loc loss:3.730868 | class loss:2.113162 \n",
      "Epoch: 5/100  | Epoch time 58.174 || Average Loss: inf\n",
      "Epoch: 6/100 | Batch 402/402 | Batch time 0.123 || Loss: 6.392639 | loc loss:4.713526 | class loss:1.679114  \n",
      "Epoch: 6/100  | Epoch time 56.726 || Average Loss: inf\n",
      "Epoch: 7/100 | Batch 402/402 | Batch time 0.080 || Loss: 6.009108 | loc loss:4.256527 | class loss:1.752581 \n",
      "Epoch: 7/100  | Epoch time 57.316 || Average Loss: inf\n",
      "Epoch: 8/100 | Batch 402/402 | Batch time 0.116 || Loss: 5.250650 | loc loss:3.210110 | class loss:2.040540 \n",
      "Epoch: 8/100  | Epoch time 56.680 || Average Loss: inf\n",
      "Epoch: 9/100 | Batch 402/402 | Batch time 0.121 || Loss: 5.282557 | loc loss:3.483898 | class loss:1.798659 \n",
      "Epoch: 9/100  | Epoch time 57.188 || Average Loss: inf\n",
      "Epoch: 10/100 | Batch 402/402 | Batch time 0.153 || Loss: 6.524471 | loc loss:4.826773 | class loss:1.697698 \n",
      "Epoch: 10/100  | Epoch time 57.572 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_010.h5<<<<<<<<<<\n",
      "Epoch: 11/100 | Batch 402/402 | Batch time 0.169 || Loss: 5.454771 | loc loss:3.884831 | class loss:1.569940 \n",
      "Epoch: 11/100  | Epoch time 57.289 || Average Loss: inf\n",
      "Epoch: 12/100 | Batch 402/402 | Batch time 0.198 || Loss: 6.074801 | loc loss:4.253697 | class loss:1.821105 \n",
      "Epoch: 12/100  | Epoch time 58.162 || Average Loss: inf\n",
      "Epoch: 13/100 | Batch 402/402 | Batch time 0.109 || Loss: 5.514575 | loc loss:3.796407 | class loss:1.718168 \n",
      "Epoch: 13/100  | Epoch time 57.779 || Average Loss: inf\n",
      "Epoch: 14/100 | Batch 402/402 | Batch time 0.120 || Loss: 5.618215 | loc loss:4.124137 | class loss:1.494077 \n",
      "Epoch: 14/100  | Epoch time 57.525 || Average Loss: inf\n",
      "Epoch: 15/100 | Batch 402/402 | Batch time 0.170 || Loss: 5.239052 | loc loss:3.675301 | class loss:1.563751 \n",
      "Epoch: 15/100  | Epoch time 59.669 || Average Loss: inf\n",
      "Epoch: 16/100 | Batch 402/402 | Batch time 0.117 || Loss: 3.194332 | loc loss:1.720241 | class loss:1.474091 \n",
      "Epoch: 16/100  | Epoch time 58.367 || Average Loss: inf\n",
      "Epoch: 17/100 | Batch 402/402 | Batch time 0.110 || Loss: 4.857679 | loc loss:3.689076 | class loss:1.168603 \n",
      "Epoch: 17/100  | Epoch time 59.834 || Average Loss: inf\n",
      "Epoch: 18/100 | Batch 402/402 | Batch time 0.129 || Loss: 7.027361 | loc loss:5.659525 | class loss:1.367836 \n",
      "Epoch: 18/100  | Epoch time 59.381 || Average Loss: inf\n",
      "Epoch: 19/100 | Batch 402/402 | Batch time 0.118 || Loss: 4.266314 | loc loss:2.843485 | class loss:1.422829  \n",
      "Epoch: 19/100  | Epoch time 58.123 || Average Loss: inf\n",
      "Epoch: 20/100 | Batch 402/402 | Batch time 0.106 || Loss: 4.181316 | loc loss:2.628664 | class loss:1.552653 \n",
      "Epoch: 20/100  | Epoch time 59.232 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_020.h5<<<<<<<<<<\n",
      "Epoch: 21/100 | Batch 402/402 | Batch time 0.116 || Loss: 4.162622 | loc loss:2.929286 | class loss:1.233336 \n",
      "Epoch: 21/100  | Epoch time 58.406 || Average Loss: inf\n",
      "Epoch: 22/100 | Batch 402/402 | Batch time 0.099 || Loss: 5.623655 | loc loss:4.487172 | class loss:1.136484 \n",
      "Epoch: 22/100  | Epoch time 58.047 || Average Loss: inf\n",
      "Epoch: 23/100 | Batch 402/402 | Batch time 0.143 || Loss: 5.033020 | loc loss:3.671783 | class loss:1.361237 \n",
      "Epoch: 23/100  | Epoch time 59.238 || Average Loss: inf\n",
      "Epoch: 24/100 | Batch 402/402 | Batch time 0.105 || Loss: 6.363848 | loc loss:4.938017 | class loss:1.425831 \n",
      "Epoch: 24/100  | Epoch time 61.250 || Average Loss: inf\n",
      "Epoch: 25/100 | Batch 402/402 | Batch time 0.101 || Loss: 3.210861 | loc loss:1.983219 | class loss:1.227642 \n",
      "Epoch: 25/100  | Epoch time 59.751 || Average Loss: inf\n",
      "Epoch: 26/100 | Batch 402/402 | Batch time 0.117 || Loss: 4.942132 | loc loss:3.781219 | class loss:1.160912 \n",
      "Epoch: 26/100  | Epoch time 59.136 || Average Loss: inf\n",
      "Epoch: 27/100 | Batch 402/402 | Batch time 0.085 || Loss: 5.074397 | loc loss:3.585213 | class loss:1.489183 \n",
      "Epoch: 27/100  | Epoch time 58.816 || Average Loss: inf\n",
      "Epoch: 28/100 | Batch 402/402 | Batch time 0.109 || Loss: 6.859098 | loc loss:5.293825 | class loss:1.565273 \n",
      "Epoch: 28/100  | Epoch time 59.740 || Average Loss: inf\n",
      "Epoch: 29/100 | Batch 402/402 | Batch time 0.138 || Loss: 6.147159 | loc loss:4.918219 | class loss:1.228940 \n",
      "Epoch: 29/100  | Epoch time 59.073 || Average Loss: inf\n",
      "Epoch: 30/100 | Batch 402/402 | Batch time 0.171 || Loss: 6.437642 | loc loss:5.064266 | class loss:1.373376 \n",
      "Epoch: 30/100  | Epoch time 61.103 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_030.h5<<<<<<<<<<\n",
      "Epoch: 31/100 | Batch 402/402 | Batch time 0.136 || Loss: 6.741727 | loc loss:5.676552 | class loss:1.065175 \n",
      "Epoch: 31/100  | Epoch time 58.861 || Average Loss: inf\n",
      "Epoch: 32/100 | Batch 402/402 | Batch time 0.077 || Loss: 4.656956 | loc loss:3.452404 | class loss:1.204552 \n",
      "Epoch: 32/100  | Epoch time 60.482 || Average Loss: inf\n",
      "Epoch: 33/100 | Batch 402/402 | Batch time 0.078 || Loss: 4.698008 | loc loss:3.488293 | class loss:1.209715 \n",
      "Epoch: 33/100  | Epoch time 59.872 || Average Loss: inf\n",
      "Epoch: 34/100 | Batch 402/402 | Batch time 0.122 || Loss: 4.403522 | loc loss:3.244412 | class loss:1.159109 \n",
      "Epoch: 34/100  | Epoch time 56.428 || Average Loss: inf\n",
      "Epoch: 35/100 | Batch 402/402 | Batch time 0.138 || Loss: 4.350946 | loc loss:3.150736 | class loss:1.200211 \n",
      "Epoch: 35/100  | Epoch time 57.870 || Average Loss: inf\n",
      "Epoch: 36/100 | Batch 402/402 | Batch time 0.145 || Loss: 5.714799 | loc loss:4.458504 | class loss:1.256295 \n",
      "Epoch: 36/100  | Epoch time 58.771 || Average Loss: inf\n",
      "Epoch: 37/100 | Batch 402/402 | Batch time 0.091 || Loss: 5.664055 | loc loss:4.338814 | class loss:1.325242 \n",
      "Epoch: 37/100  | Epoch time 56.684 || Average Loss: inf\n",
      "Epoch: 38/100 | Batch 402/402 | Batch time 0.119 || Loss: 4.950979 | loc loss:3.692004 | class loss:1.258975 \n",
      "Epoch: 38/100  | Epoch time 57.857 || Average Loss: inf\n",
      "Epoch: 39/100 | Batch 402/402 | Batch time 0.139 || Loss: 3.940356 | loc loss:2.728842 | class loss:1.211514 \n",
      "Epoch: 39/100  | Epoch time 57.757 || Average Loss: inf\n",
      "Epoch: 40/100 | Batch 402/402 | Batch time 0.137 || Loss: 5.398511 | loc loss:4.207413 | class loss:1.191098 \n",
      "Epoch: 40/100  | Epoch time 56.630 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_040.h5<<<<<<<<<<\n",
      "Epoch: 41/100 | Batch 402/402 | Batch time 0.139 || Loss: 4.524208 | loc loss:2.821246 | class loss:1.702962 \n",
      "Epoch: 41/100  | Epoch time 57.166 || Average Loss: inf\n",
      "Epoch: 42/100 | Batch 402/402 | Batch time 0.155 || Loss: 4.717388 | loc loss:3.321240 | class loss:1.396147 \n",
      "Epoch: 42/100  | Epoch time 57.386 || Average Loss: inf\n",
      "Epoch: 43/100 | Batch 402/402 | Batch time 0.117 || Loss: 3.309096 | loc loss:1.953312 | class loss:1.355784 \n",
      "Epoch: 43/100  | Epoch time 58.550 || Average Loss: inf\n",
      "Epoch: 44/100 | Batch 402/402 | Batch time 0.117 || Loss: 4.541170 | loc loss:3.510535 | class loss:1.030634 \n",
      "Epoch: 44/100  | Epoch time 57.394 || Average Loss: inf\n",
      "Epoch: 45/100 | Batch 402/402 | Batch time 0.113 || Loss: 3.791306 | loc loss:2.597622 | class loss:1.193685 \n",
      "Epoch: 45/100  | Epoch time 56.829 || Average Loss: inf\n",
      "Epoch: 46/100 | Batch 402/402 | Batch time 0.134 || Loss: 5.872817 | loc loss:4.660002 | class loss:1.212814 \n",
      "Epoch: 46/100  | Epoch time 53.492 || Average Loss: inf\n",
      "Epoch: 47/100 | Batch 402/402 | Batch time 0.118 || Loss: 5.019871 | loc loss:3.933494 | class loss:1.086378 \n",
      "Epoch: 47/100  | Epoch time 52.971 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/100 | Batch 402/402 | Batch time 0.130 || Loss: 4.965695 | loc loss:3.610611 | class loss:1.355084 \n",
      "Epoch: 48/100  | Epoch time 52.876 || Average Loss: inf\n",
      "Epoch: 49/100 | Batch 402/402 | Batch time 0.084 || Loss: inf | loc loss:inf | class loss:1.222487 :1.126836 \n",
      "Epoch: 49/100  | Epoch time 50.809 || Average Loss: inf\n",
      "Epoch: 50/100 | Batch 402/402 | Batch time 0.097 || Loss: 6.326454 | loc loss:4.909838 | class loss:1.416616 \n",
      "Epoch: 50/100  | Epoch time 51.280 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_050.h5<<<<<<<<<<\n",
      "Epoch: 51/100 | Batch 402/402 | Batch time 0.114 || Loss: 4.464693 | loc loss:3.288708 | class loss:1.175985 \n",
      "Epoch: 51/100  | Epoch time 51.690 || Average Loss: inf\n",
      "Epoch: 52/100 | Batch 402/402 | Batch time 0.107 || Loss: 2.988003 | loc loss:1.995705 | class loss:0.992298 \n",
      "Epoch: 52/100  | Epoch time 51.786 || Average Loss: inf\n",
      "Epoch: 53/100 | Batch 402/402 | Batch time 0.126 || Loss: 8.162429 | loc loss:6.991858 | class loss:1.170571 \n",
      "Epoch: 53/100  | Epoch time 51.405 || Average Loss: inf\n",
      "Epoch: 54/100 | Batch 402/402 | Batch time 0.087 || Loss: 5.038712 | loc loss:4.013071 | class loss:1.025641 \n",
      "Epoch: 54/100  | Epoch time 54.599 || Average Loss: inf\n",
      "Epoch: 55/100 | Batch 402/402 | Batch time 0.134 || Loss: 6.786366 | loc loss:5.845607 | class loss:0.940759 \n",
      "Epoch: 55/100  | Epoch time 53.581 || Average Loss: inf\n",
      "Epoch: 56/100 | Batch 402/402 | Batch time 0.104 || Loss: 4.933447 | loc loss:3.834471 | class loss:1.098976 \n",
      "Epoch: 56/100  | Epoch time 55.673 || Average Loss: inf\n",
      "Epoch: 57/100 | Batch 402/402 | Batch time 0.087 || Loss: 3.055990 | loc loss:2.124601 | class loss:0.931388 \n",
      "Epoch: 57/100  | Epoch time 54.742 || Average Loss: inf\n",
      "Epoch: 58/100 | Batch 402/402 | Batch time 0.140 || Loss: 4.932978 | loc loss:3.861523 | class loss:1.071454 \n",
      "Epoch: 58/100  | Epoch time 51.473 || Average Loss: inf\n",
      "Epoch: 59/100 | Batch 402/402 | Batch time 0.125 || Loss: 5.358133 | loc loss:4.280356 | class loss:1.077776 \n",
      "Epoch: 59/100  | Epoch time 51.694 || Average Loss: inf\n",
      "Epoch: 60/100 | Batch 402/402 | Batch time 0.096 || Loss: 3.822855 | loc loss:2.439576 | class loss:1.383279 \n",
      "Epoch: 60/100  | Epoch time 50.605 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_060.h5<<<<<<<<<<\n",
      "Epoch: 61/100 | Batch 402/402 | Batch time 0.086 || Loss: 3.942554 | loc loss:2.855526 | class loss:1.087028 \n",
      "Epoch: 61/100  | Epoch time 50.747 || Average Loss: inf\n",
      "Epoch: 62/100 | Batch 402/402 | Batch time 0.104 || Loss: 3.977359 | loc loss:3.027262 | class loss:0.950097 \n",
      "Epoch: 62/100  | Epoch time 51.434 || Average Loss: inf\n",
      "Epoch: 63/100 | Batch 402/402 | Batch time 0.114 || Loss: 3.733417 | loc loss:2.671306 | class loss:1.062111 \n",
      "Epoch: 63/100  | Epoch time 53.501 || Average Loss: inf\n",
      "Epoch: 64/100 | Batch 402/402 | Batch time 0.117 || Loss: 4.926934 | loc loss:3.769028 | class loss:1.157906 \n",
      "Epoch: 64/100  | Epoch time 52.485 || Average Loss: inf\n",
      "Epoch: 65/100 | Batch 402/402 | Batch time 0.139 || Loss: 3.120626 | loc loss:2.122310 | class loss:0.998316 \n",
      "Epoch: 65/100  | Epoch time 54.115 || Average Loss: inf\n",
      "Epoch: 66/100 | Batch 402/402 | Batch time 0.131 || Loss: 4.085287 | loc loss:2.938595 | class loss:1.146692 \n",
      "Epoch: 66/100  | Epoch time 56.278 || Average Loss: inf\n",
      "Epoch: 67/100 | Batch 402/402 | Batch time 0.099 || Loss: 3.678976 | loc loss:2.217901 | class loss:1.461076 \n",
      "Epoch: 67/100  | Epoch time 53.112 || Average Loss: inf\n",
      "Epoch: 68/100 | Batch 402/402 | Batch time 0.108 || Loss: 4.187623 | loc loss:2.928899 | class loss:1.258724 \n",
      "Epoch: 68/100  | Epoch time 54.780 || Average Loss: inf\n",
      "Epoch: 69/100 | Batch 402/402 | Batch time 0.094 || Loss: 4.430125 | loc loss:3.438251 | class loss:0.991874 \n",
      "Epoch: 69/100  | Epoch time 53.448 || Average Loss: inf\n",
      "Epoch: 70/100 | Batch 402/402 | Batch time 0.133 || Loss: 2.924930 | loc loss:1.854236 | class loss:1.070694 \n",
      "Epoch: 70/100  | Epoch time 54.509 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_070.h5<<<<<<<<<<\n",
      "Epoch: 71/100 | Batch 402/402 | Batch time 0.102 || Loss: 4.442515 | loc loss:3.249053 | class loss:1.193462 \n",
      "Epoch: 71/100  | Epoch time 52.442 || Average Loss: inf\n",
      "Epoch: 72/100 | Batch 402/402 | Batch time 0.134 || Loss: 5.856088 | loc loss:4.648313 | class loss:1.207775 \n",
      "Epoch: 72/100  | Epoch time 53.839 || Average Loss: inf\n",
      "Epoch: 73/100 | Batch 402/402 | Batch time 0.113 || Loss: 7.057972 | loc loss:5.936128 | class loss:1.121844 \n",
      "Epoch: 73/100  | Epoch time 54.587 || Average Loss: inf\n",
      "Epoch: 74/100 | Batch 402/402 | Batch time 0.149 || Loss: 5.234147 | loc loss:4.121675 | class loss:1.112473 \n",
      "Epoch: 74/100  | Epoch time 57.458 || Average Loss: inf\n",
      "Epoch: 75/100 | Batch 402/402 | Batch time 0.127 || Loss: 4.075220 | loc loss:2.967022 | class loss:1.108198 \n",
      "Epoch: 75/100  | Epoch time 52.454 || Average Loss: inf\n",
      "Epoch: 76/100 | Batch 402/402 | Batch time 0.098 || Loss: 5.019123 | loc loss:4.039895 | class loss:0.979229 \n",
      "Epoch: 76/100  | Epoch time 52.379 || Average Loss: inf\n",
      "Epoch: 77/100 | Batch 402/402 | Batch time 0.136 || Loss: 5.123920 | loc loss:3.950928 | class loss:1.172993 \n",
      "Epoch: 77/100  | Epoch time 55.680 || Average Loss: inf\n",
      "Epoch: 78/100 | Batch 402/402 | Batch time 0.111 || Loss: 6.150237 | loc loss:4.814537 | class loss:1.335700 \n",
      "Epoch: 78/100  | Epoch time 55.370 || Average Loss: inf\n",
      "Epoch: 79/100 | Batch 402/402 | Batch time 0.121 || Loss: 6.381785 | loc loss:5.140162 | class loss:1.241623 \n",
      "Epoch: 79/100  | Epoch time 56.730 || Average Loss: inf\n",
      "Epoch: 80/100 | Batch 402/402 | Batch time 0.118 || Loss: 5.353695 | loc loss:4.265927 | class loss:1.087768 \n",
      "Epoch: 80/100  | Epoch time 56.274 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_080.h5<<<<<<<<<<\n",
      "Epoch: 81/100 | Batch 402/402 | Batch time 0.125 || Loss: 5.601594 | loc loss:4.431877 | class loss:1.169717 \n",
      "Epoch: 81/100  | Epoch time 58.029 || Average Loss: inf\n",
      "Epoch: 82/100 | Batch 402/402 | Batch time 0.119 || Loss: 3.734290 | loc loss:2.788204 | class loss:0.946086 \n",
      "Epoch: 82/100  | Epoch time 55.606 || Average Loss: inf\n",
      "Epoch: 83/100 | Batch 402/402 | Batch time 0.156 || Loss: 4.738158 | loc loss:3.608044 | class loss:1.130114 \n",
      "Epoch: 83/100  | Epoch time 59.163 || Average Loss: inf\n",
      "Epoch: 84/100 | Batch 402/402 | Batch time 0.124 || Loss: 3.902710 | loc loss:2.932549 | class loss:0.970160  \n",
      "Epoch: 84/100  | Epoch time 52.380 || Average Loss: inf\n",
      "Epoch: 85/100 | Batch 402/402 | Batch time 0.119 || Loss: inf | loc loss:inf | class loss:1.309790 :1.206572 \n",
      "Epoch: 85/100  | Epoch time 53.428 || Average Loss: inf\n",
      "Epoch: 86/100 | Batch 402/402 | Batch time 0.120 || Loss: 5.127800 | loc loss:4.213012 | class loss:0.914788 \n",
      "Epoch: 86/100  | Epoch time 55.738 || Average Loss: inf\n",
      "Epoch: 87/100 | Batch 402/402 | Batch time 0.119 || Loss: 4.237452 | loc loss:3.088347 | class loss:1.149105 \n",
      "Epoch: 87/100  | Epoch time 57.008 || Average Loss: inf\n",
      "Epoch: 88/100 | Batch 402/402 | Batch time 0.108 || Loss: 5.187204 | loc loss:4.044330 | class loss:1.142875 \n",
      "Epoch: 88/100  | Epoch time 54.084 || Average Loss: inf\n",
      "Epoch: 89/100 | Batch 402/402 | Batch time 0.118 || Loss: 4.808800 | loc loss:3.415172 | class loss:1.393628 \n",
      "Epoch: 89/100  | Epoch time 55.966 || Average Loss: inf\n",
      "Epoch: 90/100 | Batch 402/402 | Batch time 0.121 || Loss: 4.678574 | loc loss:3.952656 | class loss:0.725919 \n",
      "Epoch: 90/100  | Epoch time 53.317 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_090.h5<<<<<<<<<<\n",
      "Epoch: 91/100 | Batch 402/402 | Batch time 0.121 || Loss: 4.994177 | loc loss:4.072763 | class loss:0.921413 \n",
      "Epoch: 91/100  | Epoch time 54.916 || Average Loss: inf\n",
      "Epoch: 92/100 | Batch 402/402 | Batch time 0.093 || Loss: 3.542244 | loc loss:2.623200 | class loss:0.919044 \n",
      "Epoch: 92/100  | Epoch time 57.943 || Average Loss: inf\n",
      "Epoch: 93/100 | Batch 402/402 | Batch time 0.102 || Loss: 3.937336 | loc loss:3.062743 | class loss:0.874593 \n",
      "Epoch: 93/100  | Epoch time 53.593 || Average Loss: inf\n",
      "Epoch: 94/100 | Batch 402/402 | Batch time 0.131 || Loss: 4.930524 | loc loss:3.717516 | class loss:1.213008 \n",
      "Epoch: 94/100  | Epoch time 57.586 || Average Loss: inf\n",
      "Epoch: 95/100 | Batch 402/402 | Batch time 0.143 || Loss: 3.008681 | loc loss:1.779348 | class loss:1.229333 \n",
      "Epoch: 95/100  | Epoch time 56.179 || Average Loss: inf\n",
      "Epoch: 96/100 | Batch 402/402 | Batch time 0.132 || Loss: 4.470510 | loc loss:3.316916 | class loss:1.153593 \n",
      "Epoch: 96/100  | Epoch time 56.180 || Average Loss: inf\n",
      "Epoch: 97/100 | Batch 402/402 | Batch time 0.149 || Loss: 5.262118 | loc loss:4.137714 | class loss:1.124404 \n",
      "Epoch: 97/100  | Epoch time 53.725 || Average Loss: inf\n",
      "Epoch: 98/100 | Batch 402/402 | Batch time 0.075 || Loss: 4.100863 | loc loss:2.871941 | class loss:1.228922 \n",
      "Epoch: 98/100  | Epoch time 51.524 || Average Loss: inf\n",
      "Epoch: 99/100 | Batch 402/402 | Batch time 0.112 || Loss: 4.349846 | loc loss:3.224856 | class loss:1.124991 \n",
      "Epoch: 99/100  | Epoch time 54.391 || Average Loss: inf\n",
      "Epoch: 100/100 | Batch 402/402 | Batch time 0.109 || Loss: 3.317338 | loc loss:2.332646 | class loss:0.984693 \n",
      "Epoch: 100/100  | Epoch time 55.236 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_100.h5<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(init_epoch+1,cfg['epoch']):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "\n",
    "            load_t0 = time.time()\n",
    "            total_loss, losses = train_step(inputs, labels)\n",
    "            avg_loss = (avg_loss * step + total_loss.numpy()) / (step + 1)\n",
    "            load_t1 = time.time()\n",
    "            batch_time = load_t1 - load_t0\n",
    "\n",
    "            steps =steps_per_epoch*epoch+step\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "                for k, l in losses.items():\n",
    "                    tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "                tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch + 1}/{cfg['epoch']} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch + 1}/{cfg['epoch']}  | Epoch time {(load_t1 - start):.3f} || Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/avg_loss',avg_loss,step=epoch)\n",
    "\n",
    "        if (epoch + 1) % cfg['save_freq'] == 0:\n",
    "            filepath = os.path.join(weights_dir, f'weights_epoch_{(epoch + 1):03d}.h5')\n",
    "            model.save_weights(filepath)\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\">>>>>>>>>>Save weights file at {filepath}<<<<<<<<<<\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "## (1) NMS\n",
    "Grid cell을 사용하는 Object detection의 inference 단계에서 하나의 object가 여러 개의 prior box에 걸쳐져 있을 때 가장 확률이 높은 1개의 prior box를 하나로 줄여주는 NMS(non-max suppression)이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bbox_tf(pre, priors, variances=None):\n",
    "    \"\"\"Decode locations from predictions using prior to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): location predictions for loc layers,\n",
    "            Shape: [num_prior,4]\n",
    "        prior (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_prior,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        decoded bounding box predictions xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n",
    "    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n",
    "\n",
    "    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nms(boxes, scores, nms_threshold=0.5, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = _jaccard(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predict(predictions, priors, cfg):\n",
    "    label_classes = cfg['labels_list']\n",
    "\n",
    "    bbox_regressions, confs = tf.split(predictions[0], [4, -1], axis=-1)\n",
    "    boxes = decode_bbox_tf(bbox_regressions, priors, cfg['variances'])\n",
    "\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, len(label_classes)):\n",
    "        cls_scores = confs[:, c]\n",
    "\n",
    "        score_idx = cls_scores > cfg['score_threshold']\n",
    "\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, cfg['nms_threshold'], cfg['max_number_keep'])\n",
    "\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 사진에서 얼굴 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_image(img, max_steps):\n",
    "    \"\"\"pad image to suitable shape\"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"\n",
    "        recover the padded output effect\n",
    "\n",
    "    \"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "\n",
    "    recover_xy = np.reshape(outputs[0], [-1, 2, 2]) * \\\n",
    "                 [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[0] = np.reshape(recover_xy, [-1, 4])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, boxes, classes, scores, img_height, img_width, prior_index, class_list):\n",
    "    \"\"\"\n",
    "    draw bboxes and labels\n",
    "    out:boxes,classes,scores\n",
    "    \"\"\"\n",
    "    # bbox\n",
    "\n",
    "    x1, y1, x2, y2 = int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height), \\\n",
    "                     int(boxes[prior_index][2] * img_width), int(boxes[prior_index][3] * img_height)\n",
    "    if classes[prior_index] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (0, 0, 255)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    # confidence\n",
    "\n",
    "    # if scores:\n",
    "    #   score = \"{:.4f}\".format(scores[prior_index])\n",
    "    #   class_name = class_list[classes[prior_index]]\n",
    "\n",
    "    #  cv2.putText(img, '{} {}'.format(class_name, score),\n",
    "    #              (int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height) - 4),\n",
    "    #              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path : /home/ssac21/aiffel/face_detector/checkpoints/weights_epoch_100.h5\n",
      "[*] Predict /home/ssac21/aiffel/face_detector/image.png image.. \n",
      "(256, 320, 3)\n",
      "scores:[0.99561524 0.92429984 0.90864956 0.65560645]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "global model\n",
    "min_sizes = cfg['min_sizes']\n",
    "num_cell = [len(min_sizes[k]) for k in range(len(cfg['steps']))]\n",
    "model_path = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=False)\n",
    "\n",
    "    paths = [os.path.join(model_path, path)\n",
    "             for path in os.listdir(model_path)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    print(f\"model path : {latest}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Please make sure there is at least one weights at {}'.format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"Cannot find image path from {img_path}\")\n",
    "    exit()\n",
    "print(\"[*] Predict {} image.. \".format(img_path))\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (320, 240))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "print(img.shape)\n",
    "priors, _ = prior_box(cfg, image_sizes=(img.shape[0], img.shape[1]))\n",
    "priors = tf.cast(priors, tf.float32)\n",
    "\n",
    "predictions = model.predict(img[np.newaxis, ...])\n",
    "\n",
    "boxes, classes, scores = parse_predict(predictions, priors, cfg)\n",
    "\n",
    "print(f\"scores:{scores}\")\n",
    "# recover padding effect\n",
    "boxes = recover_pad_output(boxes, pad_params)\n",
    "\n",
    "# draw and save results\n",
    "save_img_path = os.path.join('assets/out_' + os.path.basename(img_path))\n",
    "\n",
    "for prior_index in range(len(boxes)):\n",
    "    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg['labels_list'])\n",
    "\n",
    "cv2.imwrite(save_img_path, img_raw)\n",
    "cv2.imshow('results', img_raw)\n",
    "if cv2.waitKey(0) == ord('q'):\n",
    "    exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
